\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{kober2012reinforcement}
\citation{kuleshov2010algorithms}
\citation{lecun1998gradient}
\citation{bellemare12arcade}
\citation{naddaf2010game}
\newlabel{atari_game}{{1}{2}{This figure shows the basic games of the Atari Learning Environment(ALE) of reinforcement learning research. On the top left corner is famous atari 2600 game Skiing. The top right corner is Zaxxon. The down left corner is Freeway. The down right corner is KongFu Mater. These games are representatives of the testing environment}{figure.1}{}}
\citation{mohri2012foundations}
\citation{watkins1989learning}
\citation{hinton2006fast}
\citation{lecun1998gradient}
\citation{hinton2012deep}
\citation{fukushima1980neocognitron}
\citation{lecun1998gradient}
\citation{hinton1986learning}
\citation{mnih2013playing}
\citation{lenz2013deep}
\citation{lecun1998gradient}
\newlabel{cnn}{{2}{3}{This figure shows the structure of CNN, specifically LeNet-5 named after Yang LeCun. This CNN contains five layers of nuero network including two covolutional layers, two downsampling layers, one full connected layer. Along with the procedure in this figure, at first the image locally convolves with, normally, a 5x5 sliding matrix filter and generate several feature maps. After the convolution, we use some way to downsample the image. Normally some non-linear machanism(MaxPooling) is used. The previous step ensures the important features are observed by the algorithm. As long as we getting enough samples, a full layer is then applied to classify them into lables}{figure.2}{}}
\newlabel{bellman}{{4}{3}{\relax }{equation.2.2}{}}
\citation{stanley2006exploiting}
\newlabel{function_output}{{3}{4}{This figure shows the input of CNN for game breakout. The function $\varphi (\cdot )$ convert the RGB(red, green, blue ) image to gray-scale image and further downsample it to four 100x100 images. The number at the top left corner shows the current score. The number at top middle shows the lives of the agent. The only moving object white dot is a ball. And the white bar in the middle is the agent that can reflect the ball to the breaks}{figure.3}{}}
\newlabel{dqn}{{4}{4}{This figure shows the structure of the DQN. Along with steps above, at beginning, images are preprocessed before entering the CNN and then after getting the reward, we train the CNN with reward}{figure.4}{}}
\citation{mnih2013playing}
\citation{langley00}
\bibdata{seminar_report}
\bibcite{bellemare12arcade}{{1}{2013}{{{Bellemare} et~al.}}{{{Bellemare}, {Naddaf}, {Veness}, and {Bowling}}}}
\bibcite{fukushima1980neocognitron}{{2}{1980}{{Fukushima}}{{}}}
\bibcite{hinton2012deep}{{3}{2012}{{Hinton et~al.}}{{Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior, Vanhoucke, Nguyen, Sainath, et~al.}}}
\bibcite{hinton1986learning}{{4}{1986}{{Hinton \& Sejnowski}}{{Hinton and Sejnowski}}}
\bibcite{hinton2006fast}{{5}{2006}{{Hinton et~al.}}{{Hinton, Osindero, and Teh}}}
\bibcite{kober2012reinforcement}{{6}{2012}{{Kober \& Peters}}{{Kober and Peters}}}
\bibcite{kuleshov2010algorithms}{{7}{2010}{{Kuleshov \& Precup}}{{Kuleshov and Precup}}}
\bibcite{lecun1998gradient}{{8}{1998}{{LeCun et~al.}}{{LeCun, Bottou, Bengio, and Haffner}}}
\bibcite{lenz2013deep}{{9}{2013}{{Lenz et~al.}}{{Lenz, Lee, and Saxena}}}
\bibcite{mnih2013playing}{{10}{2013}{{Mnih et~al.}}{{Mnih, Kavukcuoglu, Silver, Graves, Antonoglou, Wierstra, and Riedmiller}}}
\bibcite{mohri2012foundations}{{11}{2012}{{Mohri et~al.}}{{Mohri, Rostamizadeh, and Talwalkar}}}
\bibcite{naddaf2010game}{{12}{2010}{{Naddaf}}{{}}}
\bibcite{stanley2006exploiting}{{13}{2006}{{Stanley}}{{}}}
\bibcite{watkins1989learning}{{14}{1989}{{Watkins}}{{}}}
\bibstyle{icml2013}
